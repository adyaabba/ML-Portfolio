{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isRX-62UVMFL"
      },
      "source": [
        "\n",
        "- Train on randomly generated circles (using the circle_generator function below)\n",
        "- Use 27x27 pixel images\n",
        "- Use no more than 30,000 randomly generated samples (e.g. batchsize 30 and 1000 steps, or batchsize 1000 and 30 steps, or anywhere inbetween) in training the final networks for each task\n",
        "- Use the mean_squared_error loss function\n",
        "- Fulfil the network size requirement listed in the task (can be verifired using the print_layers function, after the network is partially trained)\n",
        "\n",
        "### Task 1:\n",
        "Implement any network design, but the bottleneck must contain no more than 9 neurons.\n",
        "\n",
        "### Task 2:\n",
        "Implement any network design, but the bottleneck must contain no more than 3 neurons.\n",
        "\n",
        "\n",
        "\n",
        "#### Practicalities\n",
        "You should use this notebook for your work and upload it to  Moodle. You are expected to use TensorFlow and Keras to complete these tasks. The notebook should be self-contained and able to be executed if necessary. Marks will be awarded for (roughly equally weighted):\n",
        "- Overall notebook clarity (both in terms of good coding practice and coherent discussion)\n",
        "- Task 1 performance (0.02 is a good target cost to do better than)\n",
        "- Task 2 performance ( a good target here is left for the student to determine)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aniF-d0nVMFM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2Rs-BtlVMFO"
      },
      "outputs": [],
      "source": [
        "#A big messy function to do the training\n",
        "# model -- our keras neural model autoencoder\n",
        "# image_generator -- a function to generate random images for the training (see below for examples)\n",
        "# img_size -- the size of our image in pixels\n",
        "# batchsize -- the number of images to include in each training batch\n",
        "# steps -- the number of steps taken in the training\n",
        "#\n",
        "# returns an array of the costs\n",
        "def generate_and_train(model,image_generator,img_size,batchsize,steps):\n",
        "\n",
        "    #Generate an array of the numbers 1 to img_size and create a meshgrid from them\n",
        "    pixels=np.linspace(-1,1,img_size)\n",
        "    x,y=np.meshgrid(pixels,pixels)\n",
        "\n",
        "    #Now create a test image using 1 call to image_generator\n",
        "    #y_test=np.zeros([1,pixels,pixels,1])\n",
        "    #y_test[:,:,:,0]=image_generator(1,x,y)\n",
        "\n",
        "    #Now create the empty arrays for the images and cost\n",
        "    y_in=np.zeros([batchsize,img_size,img_size,1])\n",
        "    y_target=np.zeros([batchsize,img_size,img_size,1])\n",
        "    cost=np.zeros(steps)\n",
        "\n",
        "    #Loop through the steps, get a random batch of samples, train the model, repeat\n",
        "    for k in range(steps):\n",
        "        # produce samples:\n",
        "        y_in[:,:,:,0]=image_generator(batchsize,x,y)\n",
        "        y_target=np.copy(y_in) # autoencoder wants to reproduce its input!\n",
        "\n",
        "        # do one training step on this batch of samples:\n",
        "        cost[k]=model.train_on_batch(y_in,y_target)\n",
        "\n",
        "    return cost,y_target\n",
        "\n",
        "def get_test_image(image_generator,img_size):\n",
        "    #Generate an array of the numbers 1 to img_size and create a meshgrid from them\n",
        "    pixels=np.linspace(-1,1,img_size)\n",
        "    x,y=np.meshgrid(pixels,pixels)\n",
        "\n",
        "    #Now create a test image using 1 call to image_generator\n",
        "    y_test=np.zeros([1,img_size,img_size,1])\n",
        "    y_test[:,:,:,0]=image_generator(1,x,y)\n",
        "    return y_test\n",
        "\n",
        "# A function to generate and plot a single test image and the output of our model\n",
        "# only to be called after training the model\n",
        "def plot_test_image(model,image_generator,img_size):\n",
        "    #Get random test image\n",
        "    y_test=get_test_image(image_generator,img_size)\n",
        "\n",
        "    #Create the output image\n",
        "    y_test_out=model.predict_on_batch(y_test)\n",
        "    fig, ax = plt.subplots(1,2)\n",
        "    ax[0].imshow(y_test[0,:,:,0],origin='lower')\n",
        "    ax[0].set_title(\"Input\")\n",
        "    ax[1].imshow(y_test_out[0,:,:,0],origin='lower')\n",
        "    ax[1].set_title(\"Output\")\n",
        "\n",
        "def print_layers(network, y_in):\n",
        "    \"\"\"\n",
        "    Call this on some test images y_in, to get a print-out of\n",
        "    the layer sizes. Shapes shown are (batchsize,pixels,pixels,channels).\n",
        "    After a call to the visualization routine, y_target will contain\n",
        "    the last set of training images, so you could feed those in here.\n",
        "    \"\"\"\n",
        "    layer_features=get_layer_activations(network,y_in)\n",
        "    #print(layer_features)\n",
        "    for idx,feature in enumerate(layer_features):\n",
        "        s=np.shape(feature)\n",
        "        num_neurons = np.prod(s[1:])  # Calculate the number of neurons excluding the batch dimension\n",
        "        print(f\"Layer {idx}: {num_neurons} neurons / {s}\")\n",
        "        #print(\"Layer \"+str(idx)+\": \"+str(s[1]*s[2]*s[3])+\" neurons / \", s)\n",
        "\n",
        "def get_layer_activation_extractor(network):\n",
        "    #print(network.inputs)\n",
        "    #for layer in network.layers:\n",
        "    #    print(layer.output)\n",
        "    return(keras.Model(inputs=network.inputs,\n",
        "                            outputs=[layer.output for layer in network.layers]))\n",
        "\n",
        "def get_layer_activations(network, y_in):\n",
        "    \"\"\"\n",
        "    Call this on some test images y_in, to get the intermediate\n",
        "    layer neuron values. These are returned in a list, with one\n",
        "    entry for each layer (the entries are arrays).\n",
        "    \"\"\"\n",
        "    extractor=get_layer_activation_extractor(network)\n",
        "    #print(extractor)\n",
        "    layer_features = extractor(y_in)\n",
        "    return layer_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CRFAkLHVMFP"
      },
      "source": [
        "## Circle generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z9ezRajVMFQ"
      },
      "outputs": [],
      "source": [
        "# A simple image generator that returns an array of batchsize images\n",
        "# each image has a size of x * y pixels\n",
        "# in this image each image has a randomly placed circle (and the circle is of random size)\n",
        "def circle_generator(batchsize,x,y):\n",
        "    R=np.random.uniform(size=batchsize)\n",
        "    x0=np.random.uniform(size=batchsize,low=-1,high=1)\n",
        "    y0=np.random.uniform(size=batchsize,low=-1,high=1)\n",
        "    return( 1.0*((x[None,:,:]-x0[:,None,None])**2 + (y[None,:,:]-y0[:,None,None])**2 < R[:,None,None]**2) )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TASK 1: AUTOENCODER, 9 NEURONS, ~ 1 min 30 secs runtime\n",
        "\n",
        "# DEFINE NETWORK\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(27, 27, 1)),\n",
        "    layers.MaxPooling2D((2, 2), padding='same'),\n",
        "    layers.Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
        "\n",
        "    # Flatten and go through the Bottleneck\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(9, activation='relu'),  # Bottleneck with 9 neurons\n",
        "\n",
        "    # Decoder with Convolutional Layers\n",
        "    layers.Dense(7 * 7 * 16, activation='relu'),  # Adjusted size to fit the convolutional structure\n",
        "    layers.Reshape((7, 7, 16)),\n",
        "    layers.Conv2DTranspose(16, (3, 3), strides=2, activation='relu', padding='valid'),  # Upsample to [15, 15, 16]\n",
        "    layers.Conv2DTranspose(32, (3, 3), strides=2, activation='relu', padding='valid'),  # Upsample to [31, 31, 32]\n",
        "    layers.Cropping2D(cropping=((2, 2), (2, 2))),  # Crop to [27, 27, 32]\n",
        "    layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')  # Final output layer\n",
        "])\n",
        "\n",
        "#COMPILE AND TRAIN MODEL\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer='adam')\n",
        "\n",
        "steps=3000\n",
        "cost,y_target=generate_and_train(model,circle_generator,img_size=9*3,batchsize=10,steps=steps)\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#COST FUNCTION\n",
        "final_cost = cost[-1]\n",
        "print(f\"The final training cost is: {final_cost}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df1XmedW2eaI",
        "outputId": "df881188-a554-415a-9474-018863119a0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " max_pooling2d_4 (MaxPoolin  (None, 14, 14, 1)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 14, 14, 16)        160       \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 3136)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 9)                 28233     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 784)               7840      \n",
            "                                                                 \n",
            " reshape_4 (Reshape)         (None, 7, 7, 16)          0         \n",
            "                                                                 \n",
            " conv2d_transpose_8 (Conv2D  (None, 15, 15, 16)        2320      \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " conv2d_transpose_9 (Conv2D  (None, 31, 31, 32)        4640      \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " cropping2d_4 (Cropping2D)   (None, 27, 27, 32)        0         \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 27, 27, 1)         289       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 43482 (169.85 KB)\n",
            "Trainable params: 43482 (169.85 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "The final training cost is: 0.009443522430956364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#improving model performance with a function, ~ 1 min\n",
        "\n",
        "def prune_weights(model, threshold=0.01):\n",
        "    '''Prunes weights in the specified model by setting values below a threshold to zero. Removes neurons that don't do anything - improves performance of our model\n",
        "\n",
        "    Parameters:\n",
        "        model (keras.models.Sequential): The model whose weights are to be pruned.\n",
        "        threshold (float): The threshold below which weights are set to zero. Default is 0.01.\n",
        "\n",
        "    Returns:\n",
        "        keras.models.Sequential: The pruned model with updated weights.\n",
        "         '''\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, layers.Dense) or isinstance(layer, layers.Conv2D):\n",
        "            weights, biases = layer.get_weights()\n",
        "            pruned_weights = np.where(np.abs(weights) < threshold, 0, weights)\n",
        "            layer.set_weights([pruned_weights, biases])\n",
        "    return model\n",
        "\n",
        "\n",
        "# Then, prune the model\n",
        "pruned_model_og = prune_weights(model)\n",
        "\n",
        "pruned_model_og.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "##Retrain the Pruned Model\n",
        "pruned_cost_array_og, y_target_pruned_og = generate_and_train(pruned_model_og, circle_generator, 27, batchsize=1000, steps=30)\n",
        "\n",
        "# To check the final cost after training:\n",
        "final_cost_pruned_og = pruned_cost_array_og[-1]\n",
        "print(f\"The pruned final training cost is: {final_cost_pruned_og}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6MnlHkhiiTq",
        "outputId": "5385844a-2fad-4d46-dd09-cb083c5b06a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The pruned final training cost is: 0.007045758422464132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print_layers() to see no. of neurons per layer\n",
        "print_layers(model, y_target)\n",
        "\n",
        "#check generated circle vs output of autoencoder (9 neuron bottleneck)\n",
        "plot_test_image(pruned_model_og, circle_generator, 27)\n",
        "plt.title('Autoencoder (9 neurons)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "6GKAPtjQfWKY",
        "outputId": "27ba0161-4525-476b-def5-0d0f51907d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eaa0f2b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Layer 0: 196 neurons / (10, 14, 14, 1)\n",
            "Layer 1: 3136 neurons / (10, 14, 14, 16)\n",
            "Layer 2: 3136 neurons / (10, 3136)\n",
            "Layer 3: 3 neurons / (10, 3)\n",
            "Layer 4: 784 neurons / (10, 784)\n",
            "Layer 5: 784 neurons / (10, 7, 7, 16)\n",
            "Layer 6: 3600 neurons / (10, 15, 15, 16)\n",
            "Layer 7: 29662 neurons / (10, 31, 31, 32)\n",
            "Layer 8: 23328 neurons / (10, 27, 27, 32)\n",
            "Layer 9: 729 neurons / (10, 27, 27, 1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Autoencoder (9 neurons)')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TASK 2: AUTOENCODER, 3 NEURONS, ~ 3 min\n",
        "\n",
        "# DEFINE NETWORK\n",
        "model1 = models.Sequential([\n",
        "    layers.Input(shape=(27, 27, 1)),\n",
        "    layers.MaxPooling2D((2, 2), padding='same'),\n",
        "    layers.Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
        "\n",
        "    # Flatten and go through the Bottleneck\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(3, activation='relu'),  # Bottleneck with 3 neurons\n",
        "\n",
        "    # Decoder with Convolutional Layers\n",
        "    layers.Dense(7 * 7 * 16, activation='relu'),  # Adjusted size to fit the convolutional structure\n",
        "    layers.Reshape((7, 7, 16)),\n",
        "    layers.Conv2DTranspose(16, (3, 3), strides=2, activation='relu', padding='valid'),  # Upsample to [15, 15, 16]\n",
        "    layers.Conv2DTranspose(32, (3, 3), strides=2, activation='relu', padding='valid'),  # Upsample to [31, 31, 32]\n",
        "    layers.Cropping2D(cropping=((2, 2), (2, 2))),  # Crop to [27, 27, 32]\n",
        "    layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')  # Final output layer\n",
        "])\n",
        "\n",
        "#COMPILE AND TRAIN MODEL\n",
        "model1.compile(loss='mean_squared_error',\n",
        "              optimizer='adam')\n",
        "\n",
        "steps_new=3000\n",
        "cost_array,y_target_new =generate_and_train(model1,circle_generator,img_size=9*3,batchsize=10,steps=steps_new)\n",
        "\n",
        "\n",
        "model1.summary()\n",
        "\n",
        "\n",
        "#COST FUNCTION\n",
        "final_cost_new = cost_array[-1]\n",
        "print(f\"The final training cost is: {final_cost_new}\")\n",
        "\n",
        "\n",
        "# Then, prune the model\n",
        "pruned_model = prune_weights(model1)\n",
        "\n",
        "pruned_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "##Retrain the Pruned Model\n",
        "pruned_cost_array, y_target_pruned = generate_and_train(pruned_model, circle_generator, 27, batchsize=10, steps=steps_new)\n",
        "\n",
        "# To check the final cost after training:\n",
        "final_cost_pruned = pruned_cost_array[-1]\n",
        "print(f\"The pruned final training cost is: {final_cost_pruned}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "N9SX92_p2eqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee6dc874-feb5-47cf-f0c7-bda11c924bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " max_pooling2d_7 (MaxPoolin  (None, 14, 14, 1)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 14, 14, 16)        160       \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 3136)              0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 3)                 9411      \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 784)               3136      \n",
            "                                                                 \n",
            " reshape_7 (Reshape)         (None, 7, 7, 16)          0         \n",
            "                                                                 \n",
            " conv2d_transpose_14 (Conv2  (None, 15, 15, 16)        2320      \n",
            " DTranspose)                                                     \n",
            "                                                                 \n",
            " conv2d_transpose_15 (Conv2  (None, 31, 31, 32)        4640      \n",
            " DTranspose)                                                     \n",
            "                                                                 \n",
            " cropping2d_7 (Cropping2D)   (None, 27, 27, 32)        0         \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 27, 27, 1)         289       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19956 (77.95 KB)\n",
            "Trainable params: 19956 (77.95 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "The final training cost is: 0.011270532384514809\n",
            "The pruned final training cost is: 0.010664056986570358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print_layers() to see no. of neurons per layer\n",
        "print_layers(model1, y_target_new)\n",
        "\n",
        "#check generated circle vs output of autoencoder (3 neuron bottleneck)\n",
        "plot_test_image(pruned_model, circle_generator, 27)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "zKKq9Ma2hVuV",
        "outputId": "b96a2565-b54e-4cca-c99e-211313c39e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eaa0f2b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
            "Layer 0: 196 neurons / (10, 14, 14, 1)\n",
            "Layer 1: 3136 neurons / (10, 14, 14, 16)\n",
            "Layer 2: 3136 neurons / (10, 3136)\n",
            "Layer 3: 3 neurons / (10, 3)\n",
            "Layer 4: 784 neurons / (10, 784)\n",
            "Layer 5: 784 neurons / (10, 7, 7, 16)\n",
            "Layer 6: 3600 neurons / (10, 15, 15, 16)\n",
            "Layer 7: 29662 neurons / (10, 31, 31, 32)\n",
            "Layer 8: 23328 neurons / (10, 27, 27, 32)\n",
            "Layer 9: 729 neurons / (10, 27, 27, 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Conclusion\n",
        "\n",
        "Model architecture:The inclusion of convolutional, pooling, dense, and transpose convolutional layers in the autoencoder architecture facilitates effective feature extraction, dimensionality reduction, and reconstruction of input images. The bottleneck layer serves as the compressed representation of the input data, capturing the key features of an image.\n",
        "\n",
        "\n",
        "Bottleneck size:\n",
        "\n",
        "In Task 1, the bottleneck with 9 neurons allowed us to strike a balance between retaining essential information for effective reconstruction while still managing computational complexity within reasonable bounds. However, Task 2's constraint of a bottleneck with only 3 neurons posed a more significant challenge, as it required more aggressive compression, leading to a pronounced trade-off between preserving critical details and maintaining computational efficiency.\n",
        "\n",
        "Training performance:\n",
        "\n",
        "We observed notable differences between the two architectures. For the network with a 9-neuron bottleneck, we achieved a final training cost of 0.0094. After pruning, the cost further decreased to 0.0070, indicating successful convergence with minimal redundancy. Conversely, the network with a 3-neuron bottleneck presented greater challenges, with a final training cost of 0.0113, reflecting a more constrained architecture that struggled to capture essential features. Despite pruning, the cost remained relatively high at 0.0107, suggesting potential issues with underfitting due to excessive compression.\n",
        "\n",
        "Pruning involved removing insignificant weights from the model, aiming to reduce redundancy and improve efficiency. While pruning helped marginally lower the final training cost, it failed to address the fundamental challenge of representing complex data with a constrained bottleneck. This underscores the limitations of pruning alone in improving performance and emphasises the need for a well-designed architecture from the outset. This illustrates the delicate tradeoff between model capacity and its ability to capture essential features, emphasising the need for careful consideration of architecture design to achieve optimal performance.\n",
        "\n",
        "Generalisation:\n",
        "\n",
        "Due to its higher cost function, the 9-neuron bottleneck model offers a higher capacity to capture intricate features present in the randomly generated circles due to its larger bottleneck size. With more neurons available in the bottleneck layer, the model can encode and decode a richer representation of the input data, potentially resulting in a more faithful reconstruction of the circles during training. This enhanced capacity allows the model to better generalise to unseen data by effectively capturing the underlying patterns and variations present in the circle images, leading to improved performance in terms of reconstruction accuracy and final training cost.\n",
        "\n",
        "Task specific:\n",
        "\n",
        "The constraints posed by the circle generator function, requiring the reconstruction of 27x27 pixel images from randomly generated circles, influenced our network design choices significantly.\n",
        "\n",
        "Additionally, we adjusted batch sizes and training steps to ensure compliance with the sample size constraint while aiming for optimal convergence. By adjusting the batch size to 10 and the number of training steps to 3000, I aimed to balance computational efficiency with effective learning. Experimenting with different multiples of batch size and training steps, ensuring the total sample size remains within the specified limit of 30,000, allowed for thorough exploration of the model's learning dynamics. This iterative approach enabled me to identify the configuration that yielded the lowest final training cost, indicating a more efficient use of the available data for training the autoencoder networks.\n",
        "\n",
        "Future directions:\n",
        "\n",
        "It's worth exploring alternative network architectures, such as deeper or more complex models, to potentially improve the autoencoder networks' performance.\n",
        "Exploring deeper or more complex models involves increasing the number of layers and adjusting their width, aiming to capture intricate patterns in the data for improved performance. However, balancing model complexity with computational resources and the risk of overfitting is crucial, as deeper or more complex models may require extensive training and regularisation to prevent overfitting on limited datasets.\n",
        "\n",
        "Additionally, experimenting with different regularisation techniques, such as dropout or batch normalisation, could help address any overfitting issues encountered during training.\n",
        "\n",
        "Furthermore, investigating alternative loss functions (metrics other than the mean squared error) tailored to the specific task requirements might provide insights into further optimising the models. This could include perceptual loss for image reconstruction tasks or adversarial loss for generative tasks. By exploring and adapting loss functions to match the task requirements, one can potentially improve the model's ability to learn meaningful representations and generate more accurate outputs.\n"
      ],
      "metadata": {
        "id": "akfrt_vKhigN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MyWwTZfcwjZP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}